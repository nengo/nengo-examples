{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Spotting on Loihi\n",
    "\n",
    "This notebook provides a minimal example of a keyword spotter trained offline to recognize the keyword \"aloha\" and then run on Loihi. To keep things short for this tutorial, we'll implement a single layer of 200 neurons on the chip, and use this network to predict character transcriptions of audio signals. First we'll import some utility functions for initializing weights and for converting a nengo simulation output to text characters. \n",
    "\n",
    "The purpose of this example illustrative - since we're keeping things short and simple, we're not aiming to get a high-accuracy model; rather, we're trying to summarize the key steps in the process of building such a model. You can listen to examples of the audio input by going to the `data/audio` directory of this repository.\n",
    "\n",
    "**Software Dependencies**: Nengo DL, Nengo Loihi, Jupyter (for this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import nengo_loihi\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils import predict_text, ce_loss, weight_init, create_stream, download_and_unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup\n",
    "\n",
    "First, we set some high level parametes and load the data we'll be training the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim = 390\n",
    "out_dim = 29\n",
    "\n",
    "n_neurons = 200\n",
    "max_rate = 250\n",
    "amplitude = 1 / max_rate\n",
    "\n",
    "lifs = nengo.LIF(tau_rc=0.02, tau_ref=0.002, amplitude=amplitude)\n",
    "\n",
    "# download the data if this has not yet be done\n",
    "if not os.path.isfile('./data/train_data.pkl') or not os.path.isfile('./data/test_data.pkl'):\n",
    "    download_and_unzip('1YMMZkE0kdyaE3PbtsQGgZ59X10P6QR7b', 'speech_data.zip', './data/')\n",
    "\n",
    "with open('./data/train_data.pkl', 'rb') as pfile:\n",
    "    train_data = pickle.load(pfile)\n",
    "\n",
    "with open('./data/test_data.pkl', 'rb') as pfile:\n",
    "    test_data = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Network\n",
    "\n",
    "This is very similar to how you typically use Nengo: create collections of neurons, then connect them to input nodes to provide data to the network. We'll just add a couple of flags to configure how the network is handled by Nengo DL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network() as net:\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "\n",
    "    inp = nengo.Node(np.zeros(inp_dim))\n",
    "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1, neuron_type=lifs)\n",
    "    out = nengo.Node(size_in=out_dim)\n",
    "\n",
    "    conn_a = nengo.Connection(\n",
    "        inp, ens.neurons, transform=weight_init(shape=(n_neurons, inp_dim)))\n",
    "\n",
    "    conn_b = nengo.Connection(\n",
    "        ens.neurons, out, transform=weight_init(shape=(out_dim, n_neurons)))\n",
    "    \n",
    "    probe = nengo.Probe(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Network\n",
    "\n",
    "Now we can take our constructed model and use Nengo DL to optimize its parameters with some minimal tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Nengo DL simulator and set the minibatch size\n",
    "with nengo_dl.Simulator(net, minibatch_size=100) as sim:\n",
    "\n",
    "    # define an optimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "    \n",
    "    # specify inputs and target\n",
    "    data = {inp: train_data['inp'], probe: train_data['out']}\n",
    "    \n",
    "    # define a loss function\n",
    "    objective = {probe: ce_loss}\n",
    "\n",
    "    # optimize the model parameters\n",
    "    sim.train(data, optimizer, n_epochs=7, objective=objective)\n",
    "    \n",
    "    # collect the parameters to port to loihi\n",
    "    params = sim.get_nengo_params([ens, conn_a, conn_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rebuild the Network for Loihi\n",
    "\n",
    "Now, we reconstruct the network, initializing the ensemble and connections with the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network() as loihi_net:\n",
    "    loihi_net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    inp = nengo.Node(np.zeros(inp_dim))\n",
    "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1, neuron_type=lifs, **params[0])\n",
    "    out = nengo.Node(size_in=out_dim)\n",
    "\n",
    "    conn_a = nengo.Connection(\n",
    "        inp, ens.neurons, transform=params[1]['transform'])\n",
    "\n",
    "    conn_b = nengo.Connection(\n",
    "        ens.neurons, out, transform=params[2]['transform'])\n",
    "    \n",
    "    probe = nengo.Probe(out, synapse=0.005)\n",
    "    loihi_net.inp = inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Keyword Spotter on Loihi\n",
    "\n",
    "Here we'll use an emulator for the chip. Once the inputs are setup, it's just one line of code to run the keyword spotter on Loihi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, text in test_data[:12]:\n",
    "    n_steps = features.shape[0]\n",
    "    loihi_net.inp.output = create_stream(features)\n",
    "\n",
    "    sim = nengo_loihi.Simulator(loihi_net, target='sim')\n",
    "\n",
    "    with sim:\n",
    "        sim.run_steps(n_steps)\n",
    "        prediction = predict_text(sim, probe, n_steps)\n",
    "        print('Correct Text: %s' % text)\n",
    "        print('Predicted Text: %s' % prediction)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for non-target-word inputs, we are not aiming to get an accurate transcription. Though this model is very simple, we can build more sophisticated models that perform comparably to a standard DNN implementation in Tensorflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
